#include <iostream>
#include <fstream>
#include <vector>
#include <string>
#include <map>
#include <stdexcept>
#include <algorithm> // For std::swap

// Third-party headers
#include <nlohmann/json.hpp>
#include "onnx.proto3.pb.h" // Generated by protoc

using json = nlohmann::json;

// A simple structure to hold tensor metadata and data
struct ManualTensor {
    std::vector<int64_t> dims;
    std::vector<float> data;
};

// --- NEW HELPER FUNCTION ---
// Transposes a 2D matrix (represented as a flat vector)
ManualTensor transpose(const ManualTensor& tensor) {
    if (tensor.dims.size() != 2) {
        throw std::runtime_error("Transpose only supports 2D tensors.");
    }
    int64_t rows = tensor.dims[0];
    int64_t cols = tensor.dims[1];

    ManualTensor transposed_tensor;
    transposed_tensor.dims = {cols, rows}; // Swap dimensions
    transposed_tensor.data.resize(rows * cols);

    for (int64_t i = 0; i < rows; ++i) {
        for (int64_t j = 0; j < cols; ++j) {
            transposed_tensor.data[j * rows + i] = tensor.data[i * cols + j];
        }
    }
    return transposed_tensor;
}

// Mocked-up weights reader
std::map<std::string, ManualTensor> read_weights_file(const std::string& filepath) {
    std::cout << "WARNING: Using hardcoded weights as .pt file parsing is complex." << std::endl;

    std::map<std::string, ManualTensor> weights;
    weights["fc1.weight"] = {{32, 10}, std::vector<float>(32 * 10, 0.1f)};
    weights["fc1.bias"]   = {{32},     std::vector<float>(32, 0.2f)};
    weights["fc2.weight"] = {{5, 32},  std::vector<float>(5 * 32, 0.3f)};
    weights["fc2.bias"]   = {{5},      std::vector<float>(5, 0.4f)};

    return weights;
}

// Helper to create an ONNX initializer
void add_initializer(onnx::GraphProto* graph, const std::string& name, const ManualTensor& tensor) {
    auto* initializer = graph->add_initializer();
    initializer->set_name(name);
    initializer->set_data_type(onnx::TensorProto_DataType_FLOAT);
    for (int64_t dim : tensor.dims) {
        initializer->add_dims(dim);
    }
    initializer->set_raw_data(tensor.data.data(), tensor.data.size() * sizeof(float));
}

int main() {
    // ... (paths and loading architecture are the same)
    const std::string arch_path = "model_arch.json";
    const std::string weights_path = "model_weights.pt";
    const std::string onnx_output_path = "model_manual_export.onnx";

    std::cout << "--- Custom C++ ONNX Exporter (No LibTorch/JIT) ---" << std::endl;
    std::ifstream arch_file(arch_path);
    json arch = json::parse(arch_file);
    auto weights = read_weights_file(weights_path);

    onnx::ModelProto model_proto;
    model_proto.set_ir_version(9); // Using compatible version
    model_proto.set_producer_name("Corrected Manual Exporter");
    model_proto.add_opset_import()->set_version(14);

    onnx::GraphProto* graph = model_proto.mutable_graph();
    graph->set_name("main_graph");

    // ... (Define Model Input is the same)
    auto* input_info = graph->add_input();
    std::string current_tensor_name = "input";
    input_info->set_name(current_tensor_name);
    auto* input_type = input_info->mutable_type()->mutable_tensor_type();
    input_type->set_elem_type(onnx::TensorProto_DataType_FLOAT);
    input_type->mutable_shape()->add_dim()->set_dim_param("batch_size");
    input_type->mutable_shape()->add_dim()->set_dim_value(10);

    // 5. Build Graph from JSON Architecture
    for (const auto& layer : arch["layers"]) {
        std::string layer_name = layer["name"];
        std::string layer_type = layer["type"];

        if (layer_type == "Linear") {
            std::string weight_name = layer["params"][0]; // "fc1.weight"
            std::string bias_name = layer["params"][1];   // "fc1.bias"
            std::string matmul_out_name = layer_name + "_matmul_out";
            std::string add_out_name = layer_name + "_add_out";

            // --- FIX IS HERE ---
            // Transpose the weight tensor before adding it as an initializer.
            ManualTensor transposed_weight = transpose(weights.at(weight_name));

            // Add the TRANSPOSED weight and the regular bias as initializers
            add_initializer(graph, weight_name, transposed_weight);
            add_initializer(graph, bias_name, weights.at(bias_name));

            // Create MatMul node - it now multiplies (batch, 10) by (10, 32)
            auto* matmul_node = graph->add_node();
            matmul_node->set_op_type("MatMul");
            matmul_node->add_input(current_tensor_name);
            matmul_node->add_input(weight_name);
            matmul_node->add_output(matmul_out_name);

            // Create Add node
            auto* add_node = graph->add_node();
            add_node->set_op_type("Add");
            add_node->add_input(matmul_out_name);
            add_node->add_input(bias_name);
            add_node->add_output(add_out_name);

            current_tensor_name = add_out_name;
        } else if (layer_type == "ReLU") {
            // ... (ReLU part is fine and remains the same)
            std::string relu_out_name = layer_name + "_out";
            auto* relu_node = graph->add_node();
            relu_node->set_op_type("Relu");
            relu_node->add_input(current_tensor_name);
            relu_node->add_output(relu_out_name);
            current_tensor_name = relu_out_name;
        }
    }

    // ... (Define Model Output and Save to File are the same)
    auto* output_info = graph->add_output();
    output_info->set_name(current_tensor_name);
    auto* output_type = output_info->mutable_type()->mutable_tensor_type();
    output_type->set_elem_type(onnx::TensorProto_DataType_FLOAT);
    output_type->mutable_shape()->add_dim()->set_dim_param("batch_size");
    output_type->mutable_shape()->add_dim()->set_dim_value(5);

    std::ofstream out_stream(onnx_output_path, std::ios::out | std::ios::binary);
    model_proto.SerializeToOstream(&out_stream);

    std::cout << "Successfully created corrected ONNX model at: " << onnx_output_path << std::endl;

    return 0;
}